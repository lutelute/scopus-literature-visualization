#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
download_open_access_pdfs_fast_stdlib.py - ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹è«–æ–‡PDFé«˜é€Ÿä¸¦åˆ—å–å¾—ï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç‰ˆï¼‰
"""

import os
import json
import re
import unicodedata
import urllib.request
import urllib.parse
import urllib.error
import time
from urllib.parse import urljoin, urlparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from typing import List, Dict, Tuple

def safe_filename(title: str, maxlen: int = 120) -> str:
    """å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ"""
    SAFE_CHARS = "-_.() " + "".join(chr(c) for c in range(0x30, 0x7B) if chr(c).isalnum())
    norm = unicodedata.normalize("NFKC", title)
    s = "".join(ch for ch in norm if ch in SAFE_CHARS)
    s = re.sub(r"\s+", "_", s).strip("_")
    s = re.sub(r"_+", "_", s)[:maxlen]
    return s or "untitled"

def make_request(url: str, method: str = 'GET', timeout: int = 10) -> Tuple[bool, Dict, bytes]:
    """HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        req = urllib.request.Request(url, headers=headers)
        if method == 'HEAD':
            req.get_method = lambda: 'HEAD'
        
        with urllib.request.urlopen(req, timeout=timeout) as response:
            response_headers = dict(response.headers)
            content = response.read() if method == 'GET' else b''
            return True, response_headers, content
            
    except Exception as e:
        print(f"âŒ Request failed for {url}: {e}")
        return False, {}, b''

def check_open_access_status(crossref_data: dict) -> dict:
    """Crossrefãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã‚’ç¢ºèª"""
    oa_info = {
        'is_open_access': False,
        'license_type': None,
        'pdf_urls': [],
        'oa_locations': []
    }
    
    # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
    licenses = crossref_data.get('license', [])
    for license_info in licenses:
        if isinstance(license_info, dict):
            license_url = license_info.get('URL', '')
            if any(oa_indicator in license_url.lower() for oa_indicator in 
                   ['creativecommons', 'cc-by', 'open-access']):
                oa_info['is_open_access'] = True
                oa_info['license_type'] = license_url
                break
    
    # ãƒªãƒ³ã‚¯æƒ…å ±ã‹ã‚‰PDF URLã‚’æ¢ã™
    links = crossref_data.get('link', [])
    for link in links:
        if isinstance(link, dict):
            content_type = link.get('content-type', '')
            url = link.get('URL', '')
            if content_type == 'application/pdf' or 'pdf' in url.lower():
                oa_info['pdf_urls'].append(url)
    
    # URLãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯
    main_url = crossref_data.get('URL', '')
    if main_url:
        oa_info['oa_locations'].append(main_url)
    
    return oa_info

def find_pdf_urls_from_doi(doi: str) -> List[str]:
    """DOIã‹ã‚‰è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã§PDF URLã‚’æ¢ç´¢"""
    pdf_urls = []
    
    # 1. Unpaywall API (ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±)
    try:
        unpaywall_url = f"https://api.unpaywall.org/v2/{doi}?email=research@example.com"
        success, headers, content = make_request(unpaywall_url, timeout=5)
        if success and content:
            data = json.loads(content.decode('utf-8'))
            if data.get('is_oa', False):
                # ãƒ™ã‚¹ãƒˆOA locationã‚’å–å¾—
                best_oa = data.get('best_oa_location')
                if best_oa and best_oa.get('url_for_pdf'):
                    pdf_urls.append(best_oa['url_for_pdf'])
                
                # ãã®ä»–ã®OA locationsã‚‚å–å¾—
                oa_locations = data.get('oa_locations', [])
                for location in oa_locations:
                    if location.get('url_for_pdf'):
                        pdf_urls.append(location['url_for_pdf'])
        time.sleep(0.1)  # APIåˆ¶é™å¯¾å¿œ
    except Exception as e:
        print(f"Unpaywall API error for {doi}: {e}")
    
    # 2. DOIç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§PDFãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
    try:
        doi_url = f"https://doi.org/{doi}"
        
        # urllib ã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’è¿½è·¡
        redirected_url = doi_url
        try:
            req = urllib.request.Request(doi_url, headers={
                'User-Agent': 'Mozilla/5.0 (Academic Research Bot 1.0)'
            })
            with urllib.request.urlopen(req, timeout=5) as response:
                redirected_url = response.url
        except:
            pass
        
        # IEEE, arXiv, PLoS ONEç­‰ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        if any(pattern in redirected_url.lower() for pattern in 
               ['arxiv.org', 'plos', 'biomedcentral', 'frontiersin', 'mdpi.com', 'ieee']):
            # PDF URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            pdf_patterns = [
                redirected_url.replace('/abs/', '/pdf/') + '.pdf',  # arXiv
                redirected_url + '.pdf',  # ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                redirected_url.replace('/article/', '/pdf/'),  # å­¦è¡“èªŒãƒ‘ã‚¿ãƒ¼ãƒ³
            ]
            pdf_urls.extend(pdf_patterns)
        time.sleep(0.1)
    except Exception as e:
        print(f"DOI redirect check error for {doi}: {e}")
    
    return list(set(pdf_urls))  # é‡è¤‡å‰Šé™¤

def download_pdf_fast(url: str, filepath: str) -> bool:
    """PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        req = urllib.request.Request(url, headers=headers)
        
        with urllib.request.urlopen(req, timeout=30) as response:
            # Content-Type ã‚’ãƒã‚§ãƒƒã‚¯
            content_type = response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and 'application/octet-stream' not in content_type:
                print(f"âŒ Not a PDF file: {content_type}")
                return False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å°100KBã€æœ€å¤§50MBï¼‰
            content_length = response.headers.get('content-length')
            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb < 0.1 or size_mb > 50:
                    print(f"âŒ File size out of range: {size_mb:.1f}MB")
                    return False
            
            # é«˜é€Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            with open(filepath, 'wb') as f:
                while True:
                    chunk = response.read(64*1024)  # 64KB chunks
                    if not chunk:
                        break
                    f.write(chunk)
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(filepath)
        if file_size < 100 * 1024:  # 100KBæœªæº€
            os.remove(filepath)
            print(f"âŒ Downloaded file too small: {file_size} bytes")
            return False
        
        print(f"âœ… Successfully downloaded: {os.path.basename(filepath)} ({file_size/1024:.0f}KB)")
        return True
        
    except Exception as e:
        print(f"âŒ Download failed from {url}: {e}")
        if os.path.exists(filepath):
            os.remove(filepath)
        return False

def add_pdf_embed_to_markdown(md_path: str, pdf_filename: str) -> None:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«PDFåŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ """
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # PDFåŸ‹ã‚è¾¼ã¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if "## PDF" in content:
            return  # æ—¢ã«è¿½åŠ æ¸ˆã¿
        
        # PDFåŸ‹ã‚è¾¼ã¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        pdf_section = f"""

## PDF

**ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆPDF**: [ğŸ“„ {pdf_filename}](PDF/{pdf_filename})

<embed src="PDF/{pdf_filename}" type="application/pdf" width="100%" height="600px" />

*ãƒ–ãƒ©ã‚¦ã‚¶ã§PDFãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚*"""
        
        # å‚è€ƒæ–‡çŒ®ã®å‰ã«æŒ¿å…¥
        if "## å‚è€ƒæ–‡çŒ®" in content:
            content = content.replace("## å‚è€ƒæ–‡çŒ®", pdf_section + "\n\n## å‚è€ƒæ–‡çŒ®")
        else:
            content += pdf_section
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“ Added PDF embed to: {os.path.basename(md_path)}")
        
    except Exception as e:
        print(f"âŒ Error adding PDF embed to {md_path}: {e}")

def process_json_for_pdf(json_path: str, pdf_dir: str, md_dir: str) -> Tuple[bool, str]:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        title = data.get('title', 'untitled')
        doi = data.get('doi', '')
        
        if not doi:
            return False, f"No DOI found for: {title}"
        
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        safe_title = safe_filename(title)
        pdf_filename = f"{safe_title}.pdf"
        pdf_path = os.path.join(pdf_dir, pdf_filename)
        md_path = os.path.join(md_dir, f"{safe_title}.md")
        
        # æ—¢ã«PDFãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(pdf_path):
            return False, f"PDF already exists: {pdf_filename}"
        
        thread_id = threading.current_thread().name
        print(f"ğŸ” [{thread_id}] Processing: {title[:50]}...")
        print(f"ğŸ“‹ [{thread_id}] DOI: {doi}")
        
        # Crossrefãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã‚’ç¢ºèª
        crossref_data = data.get('_crossref_full', {})
        oa_info = check_open_access_status(crossref_data)
        
        # PDF URL ã‚’æ¢ç´¢
        pdf_urls = find_pdf_urls_from_doi(doi)
        pdf_urls.extend(oa_info['pdf_urls'])
        
        if not pdf_urls:
            return False, f"No PDF URLs found for: {title}"
        
        # PDF ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ
        for i, url in enumerate(pdf_urls[:3]):  # æœ€å¤§3ã¤ã®URLã‚’è©¦è¡Œ
            print(f"ğŸ”„ [{thread_id}] Trying URL {i+1}/{min(3, len(pdf_urls))}: {url[:80]}...")
            if download_pdf_fast(url, pdf_path):
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã€Markdownã«PDFåŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ 
                if os.path.exists(md_path):
                    add_pdf_embed_to_markdown(md_path, pdf_filename)
                return True, f"Successfully downloaded: {pdf_filename}"
            time.sleep(0.5)  # è©¦è¡Œé–“éš”
        
        return False, f"All download attempts failed for: {title}"
        
    except Exception as e:
        return False, f"Error processing {json_path}: {e}"

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆthreadingç‰ˆï¼‰"""
    print("ğŸš€ ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹PDFé«˜é€Ÿä¸¦åˆ—å–å¾—é–‹å§‹ï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç‰ˆï¼‰...")
    start_time = time.time()
    
    base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    json_dir = os.path.join(base, "JSON_folder")
    md_dir = os.path.join(base, "md_folder")
    pdf_dir = os.path.join(base, "PDF")
    
    # PDF ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(pdf_dir, exist_ok=True)
    
    # å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]
    
    print(f"ğŸ“Š Processing {len(json_files)} files with parallel threads...")
    
    # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œï¼ˆæœ€å¤§åŒæ™‚ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼‰
    max_workers = min(8, len(json_files))  # CPUæ•°ã«å¿œã˜ã¦èª¿æ•´
    success_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¿ã‚¹ã‚¯ã¨ã—ã¦æå‡º
        future_to_file = {}
        for json_file in json_files:
            json_path = os.path.join(json_dir, json_file)
            future = executor.submit(process_json_for_pdf, json_path, pdf_dir, md_dir)
            future_to_file[future] = json_file
        
        # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
        completed = 0
        for future in as_completed(future_to_file):
            json_file = future_to_file[future]
            completed += 1
            
            try:
                success, message = future.result()
                if success:
                    success_count += 1
                    print(f"âœ… [{completed}/{len(json_files)}] {message}")
                else:
                    print(f"â„¹ï¸  [{completed}/{len(json_files)}] {message}")
            except Exception as e:
                print(f"âŒ [{completed}/{len(json_files)}] Error with {json_file}: {e}")
            
            # é€²è¡ŒçŠ¶æ³è¡¨ç¤º
            if completed % 5 == 0 or completed == len(json_files):
                elapsed = time.time() - start_time
                rate = completed / elapsed if elapsed > 0 else 0
                print(f"â³ Progress: {completed}/{len(json_files)} files | {elapsed:.1f}s | {rate:.1f} files/sec")
    
    # çµæœé›†è¨ˆ
    total_pdfs = len([f for f in os.listdir(pdf_dir) if f.endswith('.pdf')])
    end_time = time.time()
    elapsed = end_time - start_time
    
    print(f"\nğŸ‰ PDFå–å¾—å®Œäº†!")
    print(f"ğŸ“ˆ å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’")
    print(f"ğŸ“Š æ–°è¦PDFå–å¾—: {success_count}ä»¶")
    print(f"ğŸ“ ç·PDFæ•°: {total_pdfs}ä»¶")
    print(f"ğŸ“‚ PDFãƒ•ã‚©ãƒ«ãƒ€: {pdf_dir}")
    print(f"âš¡ å‡¦ç†é€Ÿåº¦: {len(json_files)/elapsed:.1f} files/sec")
    print(f"ğŸ§µ ä¸¦åˆ—åº¦: {max_workers} threads")

if __name__ == "__main__":
    main()