#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
download_open_access_pdfs.py - ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹è«–æ–‡ã®PDFè‡ªå‹•å–å¾—
"""

import os
import json
import re
import unicodedata
import requests
import time
from urllib.parse import urljoin, urlparse
from pathlib import Path

def safe_filename(title: str, maxlen: int = 120) -> str:
    """å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ"""
    SAFE_CHARS = "-_.() " + "".join(chr(c) for c in range(0x30, 0x7B) if chr(c).isalnum())
    norm = unicodedata.normalize("NFKC", title)
    s = "".join(ch for ch in norm if ch in SAFE_CHARS)
    s = re.sub(r"\s+", "_", s).strip("_")
    s = re.sub(r"_+", "_", s)[:maxlen]
    return s or "untitled"

def check_open_access_status(crossref_data: dict) -> dict:
    """Crossrefãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã‚’ç¢ºèª"""
    oa_info = {
        'is_open_access': False,
        'license_type': None,
        'pdf_urls': [],
        'oa_locations': []
    }
    
    # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
    licenses = crossref_data.get('license', [])
    for license_info in licenses:
        if isinstance(license_info, dict):
            license_url = license_info.get('URL', '')
            if any(oa_indicator in license_url.lower() for oa_indicator in 
                   ['creativecommons', 'cc-by', 'open-access']):
                oa_info['is_open_access'] = True
                oa_info['license_type'] = license_url
                break
    
    # ãƒªãƒ³ã‚¯æƒ…å ±ã‹ã‚‰PDF URLã‚’æ¢ã™
    links = crossref_data.get('link', [])
    for link in links:
        if isinstance(link, dict):
            content_type = link.get('content-type', '')
            url = link.get('URL', '')
            if content_type == 'application/pdf' or 'pdf' in url.lower():
                oa_info['pdf_urls'].append(url)
    
    # URLãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯
    main_url = crossref_data.get('URL', '')
    if main_url:
        oa_info['oa_locations'].append(main_url)
    
    return oa_info

def find_pdf_urls_from_doi(doi: str) -> list:
    """DOIã‹ã‚‰è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã§PDF URLã‚’æ¢ç´¢"""
    pdf_urls = []
    
    # 1. Unpaywall API (ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±)
    try:
        unpaywall_url = f"https://api.unpaywall.org/v2/{doi}?email=research@example.com"
        response = requests.get(unpaywall_url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data.get('is_oa', False):
                # ãƒ™ã‚¹ãƒˆOA locationã‚’å–å¾—
                best_oa = data.get('best_oa_location')
                if best_oa and best_oa.get('url_for_pdf'):
                    pdf_urls.append(best_oa['url_for_pdf'])
                
                # ãã®ä»–ã®OA locationsã‚‚å–å¾—
                oa_locations = data.get('oa_locations', [])
                for location in oa_locations:
                    if location.get('url_for_pdf'):
                        pdf_urls.append(location['url_for_pdf'])
        time.sleep(0.1)  # APIåˆ¶é™å¯¾å¿œ
    except Exception as e:
        print(f"Unpaywall API error for {doi}: {e}")
    
    # 2. DOIç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§PDFãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
    try:
        doi_url = f"https://doi.org/{doi}"
        response = requests.head(doi_url, allow_redirects=True, timeout=10)
        final_url = response.url
        
        # IEEE, arXiv, PLoS ONEç­‰ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        if any(pattern in final_url.lower() for pattern in 
               ['arxiv.org', 'plos', 'biomedcentral', 'frontiersin', 'mdpi.com', 'ieee']):
            # PDF URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            pdf_patterns = [
                final_url.replace('/abs/', '/pdf/') + '.pdf',  # arXiv
                final_url + '.pdf',  # ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                final_url.replace('/article/', '/pdf/'),  # å­¦è¡“èªŒãƒ‘ã‚¿ãƒ¼ãƒ³
            ]
            pdf_urls.extend(pdf_patterns)
        time.sleep(0.1)
    except Exception as e:
        print(f"DOI redirect check error for {doi}: {e}")
    
    return list(set(pdf_urls))  # é‡è¤‡å‰Šé™¤

def download_pdf(url: str, filepath: str) -> bool:
    """PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30, stream=True)
        response.raise_for_status()
        
        # Content-Type ã‚’ãƒã‚§ãƒƒã‚¯
        content_type = response.headers.get('content-type', '').lower()
        if 'pdf' not in content_type and 'application/octet-stream' not in content_type:
            print(f"Not a PDF file: {content_type}")
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å°100KBã€æœ€å¤§50MBï¼‰
        content_length = response.headers.get('content-length')
        if content_length:
            size_mb = int(content_length) / (1024 * 1024)
            if size_mb < 0.1 or size_mb > 50:
                print(f"File size out of range: {size_mb:.1f}MB")
                return False
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(filepath)
        if file_size < 100 * 1024:  # 100KBæœªæº€
            os.remove(filepath)
            print(f"Downloaded file too small: {file_size} bytes")
            return False
        
        print(f"Successfully downloaded: {os.path.basename(filepath)} ({file_size/1024:.0f}KB)")
        return True
        
    except Exception as e:
        print(f"Download failed from {url}: {e}")
        if os.path.exists(filepath):
            os.remove(filepath)
        return False

def add_pdf_embed_to_markdown(md_path: str, pdf_filename: str) -> None:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«PDFåŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ """
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # PDFåŸ‹ã‚è¾¼ã¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if "## PDF" in content:
            return  # æ—¢ã«è¿½åŠ æ¸ˆã¿
        
        # PDFåŸ‹ã‚è¾¼ã¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        pdf_section = f"""

## PDF

**ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆPDF**: [ğŸ“„ {pdf_filename}](PDF/{pdf_filename})

<embed src="PDF/{pdf_filename}" type="application/pdf" width="100%" height="600px" />

*ãƒ–ãƒ©ã‚¦ã‚¶ã§PDFãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚*"""
        
        # å‚è€ƒæ–‡çŒ®ã®å‰ã«æŒ¿å…¥
        if "## å‚è€ƒæ–‡çŒ®" in content:
            content = content.replace("## å‚è€ƒæ–‡çŒ®", pdf_section + "\n\n## å‚è€ƒæ–‡çŒ®")
        else:
            content += pdf_section
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"Added PDF embed to: {os.path.basename(md_path)}")
        
    except Exception as e:
        print(f"Error adding PDF embed to {md_path}: {e}")

def process_json_for_pdf(json_path: str, pdf_dir: str, md_dir: str) -> None:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        title = data.get('title', 'untitled')
        doi = data.get('doi', '')
        
        if not doi:
            print(f"No DOI found for: {title}")
            return
        
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        safe_title = safe_filename(title)
        pdf_filename = f"{safe_title}.pdf"
        pdf_path = os.path.join(pdf_dir, pdf_filename)
        md_path = os.path.join(md_dir, f"{safe_title}.md")
        
        # æ—¢ã«PDFãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(pdf_path):
            print(f"PDF already exists: {pdf_filename}")
            return
        
        print(f"Processing: {title}")
        print(f"DOI: {doi}")
        
        # Crossrefãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã‚’ç¢ºèª
        crossref_data = data.get('_crossref_full', {})
        oa_info = check_open_access_status(crossref_data)
        
        # PDF URL ã‚’æ¢ç´¢
        pdf_urls = find_pdf_urls_from_doi(doi)
        pdf_urls.extend(oa_info['pdf_urls'])
        
        if not pdf_urls:
            print(f"No PDF URLs found for: {title}")
            return
        
        # PDF ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ
        download_success = False
        for url in pdf_urls[:3]:  # æœ€å¤§3ã¤ã®URLã‚’è©¦è¡Œ
            print(f"Trying to download from: {url}")
            if download_pdf(url, pdf_path):
                download_success = True
                break
            time.sleep(1)  # è©¦è¡Œé–“éš”
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã€Markdownã«PDFåŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ 
        if download_success and os.path.exists(md_path):
            add_pdf_embed_to_markdown(md_path, pdf_filename)
        
    except Exception as e:
        print(f"Error processing {json_path}: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹PDFè‡ªå‹•å–å¾—é–‹å§‹...")
    
    base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    json_dir = os.path.join(base, "JSON_folder")
    md_dir = os.path.join(base, "md_folder")
    pdf_dir = os.path.join(base, "PDF")
    
    # PDF ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(pdf_dir, exist_ok=True)
    
    # å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]
    
    success_count = 0
    for json_file in json_files:
        json_path = os.path.join(json_dir, json_file)
        try:
            initial_pdf_count = len([f for f in os.listdir(pdf_dir) if f.endswith('.pdf')])
            process_json_for_pdf(json_path, pdf_dir, md_dir)
            final_pdf_count = len([f for f in os.listdir(pdf_dir) if f.endswith('.pdf')])
            
            if final_pdf_count > initial_pdf_count:
                success_count += 1
                
        except Exception as e:
            print(f"Error with {json_file}: {e}")
        
        time.sleep(2)  # APIåˆ¶é™ã¨ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
    
    total_pdfs = len([f for f in os.listdir(pdf_dir) if f.endswith('.pdf')])
    print(f"\nPDFå–å¾—å®Œäº†: {success_count}ä»¶ã®æ–°è¦PDFå–å¾—")
    print(f"ç·PDFæ•°: {total_pdfs}ä»¶")
    print(f"PDFãƒ•ã‚©ãƒ«ãƒ€: {pdf_dir}")

if __name__ == "__main__":
    main()