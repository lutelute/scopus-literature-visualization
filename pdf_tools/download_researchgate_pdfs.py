#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
download_researchgate_pdfs.py - ResearchGateã‹ã‚‰ã®ç©æ¥µçš„PDFå–å¾—
"""

import os
import json
import re
import unicodedata
import urllib.request
import urllib.parse
import urllib.error
import time
import random
from urllib.parse import urljoin, urlparse, quote
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from typing import List, Dict, Tuple

def safe_filename(title: str, maxlen: int = 120) -> str:
    """å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ"""
    SAFE_CHARS = "-_.() " + "".join(chr(c) for c in range(0x30, 0x7B) if chr(c).isalnum())
    norm = unicodedata.normalize("NFKC", title)
    s = "".join(ch for ch in norm if ch in SAFE_CHARS)
    s = re.sub(r"\s+", "_", s).strip("_")
    s = re.sub(r"_+", "_", s)[:maxlen]
    return s or "untitled"

def get_random_headers() -> Dict[str, str]:
    """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã¨ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
    ]
    
    return {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

def search_researchgate_for_paper(title: str, doi: str = "", authors: List[str] = None) -> List[str]:
    """ResearchGateã§è«–æ–‡ã‚’æ¤œç´¢ã—ã¦PDF URLã‚’å–å¾—"""
    potential_urls = []
    
    try:
        # æ¤œç´¢ã‚¯ã‚¨ãƒªä½œæˆ
        search_terms = []
        if title:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            title_words = re.findall(r'\b\w{4,}\b', title.lower())
            search_terms.extend(title_words[:5])  # æœ€åˆã®5ã¤ã®é‡è¦ãªå˜èª
        
        if doi:
            search_terms.append(doi)
        
        if authors:
            # è‘—è€…ã®å§“ã‚’è¿½åŠ 
            for author in authors[:2]:  # æœ€åˆã®2äººã®è‘—è€…
                if isinstance(author, str):
                    name_parts = author.split()
                    if name_parts:
                        search_terms.append(name_parts[-1])  # å§“
        
        # æ¤œç´¢URLæ§‹ç¯‰
        query = " ".join(search_terms[:8])  # æœ€å¤§8ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        encoded_query = quote(query)
        search_url = f"https://www.researchgate.net/search?q={encoded_query}"
        
        headers = get_random_headers()
        req = urllib.request.Request(search_url, headers=headers)
        
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8', errors='ignore')
            
            # ResearchGateã®è«–æ–‡ãƒšãƒ¼ã‚¸URLã‚’æŠ½å‡º
            publication_urls = re.findall(r'href="(/publication/\d+[^"]*)"', content)
            
            for url_path in publication_urls[:5]:  # æœ€åˆã®5ã¤ã®çµæœ
                full_url = f"https://www.researchgate.net{url_path}"
                potential_urls.append(full_url)
        
        time.sleep(random.uniform(1, 3))  # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“
        
    except Exception as e:
        print(f"[NG] ResearchGate search error: {e}")
    
    return potential_urls

def extract_pdf_from_researchgate_page(page_url: str) -> List[str]:
    """ResearchGateã®è«–æ–‡ãƒšãƒ¼ã‚¸ã‹ã‚‰PDF URLã‚’æŠ½å‡º"""
    pdf_urls = []
    
    try:
        headers = get_random_headers()
        req = urllib.request.Request(page_url, headers=headers)
        
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8', errors='ignore')
            
            # ResearchGateã®PDF URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
            pdf_patterns = [
                r'href="([^"]*\.pdf[^"]*)"',  # ç›´æ¥PDF ãƒªãƒ³ã‚¯
                r'data-url="([^"]*\.pdf[^"]*)"',  # data-urlå±æ€§ã®PDF
                r'src="([^"]*\.pdf[^"]*)"',  # srcå±æ€§ã®PDF
                r'"pdf_url":"([^"]*)"',  # JSONå†…ã®PDF URL
                r'"fullTextUrl":"([^"]*)"',  # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆURL
                r'href="(/publication/\d+[^"]*\.pdf[^"]*)"',  # ç›¸å¯¾ãƒ‘ã‚¹ã®PDF
            ]
            
            for pattern in pdf_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if match.startswith('/'):
                        pdf_url = f"https://www.researchgate.net{match}"
                    else:
                        pdf_url = match
                    
                    # ä¸é©åˆ‡ãªURLã‚’é™¤å¤–
                    if not any(skip in pdf_url.lower() for skip in ['thumbnail', 'preview', 'icon']):
                        pdf_urls.append(pdf_url)
            
            # ResearchGateã®ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã‚‚æ¢ã™
            download_patterns = [
                r'"downloadUrl":"([^"]*)"',
                r'data-download-url="([^"]*)"',
                r'href="([^"]*download[^"]*)"'
            ]
            
            for pattern in download_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if match.startswith('/'):
                        download_url = f"https://www.researchgate.net{match}"
                    else:
                        download_url = match
                    pdf_urls.append(download_url)
        
        time.sleep(random.uniform(0.5, 2))  # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“
        
    except Exception as e:
        print(f"[NG] Error extracting PDF from {page_url}: {e}")
    
    return list(set(pdf_urls))  # é‡è¤‡å‰Šé™¤

def download_pdf_from_researchgate(url: str, filepath: str) -> bool:
    """ResearchGateã‹ã‚‰PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        headers = get_random_headers()
        headers['Referer'] = 'https://www.researchgate.net/'
        
        req = urllib.request.Request(url, headers=headers)
        
        with urllib.request.urlopen(req, timeout=30) as response:
            # Content-Type ãƒã‚§ãƒƒã‚¯
            content_type = response.headers.get('content-type', '').lower()
            if 'pdf' not in content_type and 'application/octet-stream' not in content_type:
                # HTMLãƒšãƒ¼ã‚¸ã®å ´åˆã€ã•ã‚‰ã«PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
                if 'text/html' in content_type:
                    content = response.read().decode('utf-8', errors='ignore')
                    pdf_links = re.findall(r'href="([^"]*\.pdf[^"]*)"', content)
                    if pdf_links:
                        # æœ€åˆã®PDFãƒªãƒ³ã‚¯ã‚’è©¦è¡Œ
                        return download_pdf_from_researchgate(pdf_links[0], filepath)
                print(f"[NG] Not a PDF file: {content_type}")
                return False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            content_length = response.headers.get('content-length')
            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb < 0.1 or size_mb > 100:  # ResearchGateã¯å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚ã‚‹
                    print(f"[NG] File size out of range: {size_mb:.1f}MB")
                    return False
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            with open(filepath, 'wb') as f:
                while True:
                    chunk = response.read(64*1024)  # 64KB chunks
                    if not chunk:
                        break
                    f.write(chunk)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæœ€çµ‚ãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(filepath)
        if file_size < 100 * 1024:  # 100KBæœªæº€
            os.remove(filepath)
            print(f"[NG] Downloaded file too small: {file_size} bytes")
            return False
        
        print(f"[OK] Successfully downloaded from ResearchGate: {os.path.basename(filepath)} ({file_size/1024:.0f}KB)")
        return True
        
    except Exception as e:
        print(f"[NG] Download failed from {url}: {e}")
        if os.path.exists(filepath):
            os.remove(filepath)
        return False

def add_pdf_embed_to_markdown(md_path: str, pdf_filename: str) -> None:
    """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«PDFåŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ """
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # PDFåŸ‹ã‚è¾¼ã¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if "## PDF" in content:
            return  # æ—¢ã«è¿½åŠ æ¸ˆã¿
        
        # PDFåŸ‹ã‚è¾¼ã¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        pdf_section = f"""

## PDF

**ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆPDF**: [[FILE] {pdf_filename}](PDF/{pdf_filename})

<embed src="PDF/{pdf_filename}" type="application/pdf" width="100%" height="600px" />

*ãƒ–ãƒ©ã‚¦ã‚¶ã§PDFãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚*"""
        
        # å‚è€ƒæ–‡çŒ®ã®å‰ã«æŒ¿å…¥
        if "## å‚è€ƒæ–‡çŒ®" in content:
            content = content.replace("## å‚è€ƒæ–‡çŒ®", pdf_section + "\n\n## å‚è€ƒæ–‡çŒ®")
        else:
            content += pdf_section
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“ Added PDF embed to: {os.path.basename(md_path)}")
        
    except Exception as e:
        print(f"[NG] Error adding PDF embed to {md_path}: {e}")

def process_json_for_researchgate_pdf(json_path: str, pdf_dir: str, md_dir: str) -> Tuple[bool, str]:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ResearchGateã‹ã‚‰PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        title = data.get('title', 'untitled')
        doi = data.get('doi', '')
        authors = data.get('authors', [])
        
        if not title or title == 'untitled':
            return False, f"No valid title found"
        
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        safe_title = safe_filename(title)
        pdf_filename = f"{safe_title}.pdf"
        pdf_path = os.path.join(pdf_dir, pdf_filename)
        md_path = os.path.join(md_dir, f"{safe_title}.md")
        
        # æ—¢ã«PDFãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(pdf_path):
            return False, f"PDF already exists: {pdf_filename}"
        
        thread_id = threading.current_thread().name
        print(f"[INFO] [{thread_id}] Searching ResearchGate for: {title[:50]}...")
        
        # ResearchGateã§æ¤œç´¢
        researchgate_pages = search_researchgate_for_paper(title, doi, authors)
        
        if not researchgate_pages:
            return False, f"No ResearchGate pages found for: {title}"
        
        print(f"ğŸ“‹ [{thread_id}] Found {len(researchgate_pages)} ResearchGate pages")
        
        # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰PDF URLã‚’æŠ½å‡ºã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ
        for i, page_url in enumerate(researchgate_pages[:3]):  # æœ€å¤§3ãƒšãƒ¼ã‚¸ã‚’è©¦è¡Œ
            print(f"[PROC] [{thread_id}] Checking page {i+1}: {page_url}")
            
            pdf_urls = extract_pdf_from_researchgate_page(page_url)
            
            for j, pdf_url in enumerate(pdf_urls[:3]):  # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€å¤§3ã¤ã®PDF URL
                print(f"ğŸ“¥ [{thread_id}] Trying PDF {j+1}: {pdf_url[:80]}...")
                
                if download_pdf_from_researchgate(pdf_url, pdf_path):
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã€Markdownã«PDFåŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ 
                    if os.path.exists(md_path):
                        add_pdf_embed_to_markdown(md_path, pdf_filename)
                    return True, f"Successfully downloaded from ResearchGate: {pdf_filename}"
                
                time.sleep(random.uniform(1, 3))  # è©¦è¡Œé–“éš”
        
        return False, f"All ResearchGate download attempts failed for: {title}"
        
    except Exception as e:
        return False, f"Error processing {json_path}: {e}"

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("[START] ResearchGateç©æ¥µçš„PDFå–å¾—é–‹å§‹...")
    start_time = time.time()
    
    base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    json_dir = os.path.join(base, "JSON_folder")
    md_dir = os.path.join(base, "md_folder")
    pdf_dir = os.path.join(base, "PDF")
    
    # PDF ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(pdf_dir, exist_ok=True)
    
    # å…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]
    
    print(f"[DATA] Processing {len(json_files)} files with ResearchGate search...")
    
    # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œï¼ˆResearchGateã®è² è·ã‚’è€ƒæ…®ã—ã¦åˆ¶é™ï¼‰
    max_workers = min(3, len(json_files))  # æœ€å¤§3ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è€ƒæ…®ï¼‰
    success_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¿ã‚¹ã‚¯ã¨ã—ã¦æå‡º
        future_to_file = {}
        for json_file in json_files:
            json_path = os.path.join(json_dir, json_file)
            future = executor.submit(process_json_for_researchgate_pdf, json_path, pdf_dir, md_dir)
            future_to_file[future] = json_file
        
        # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
        completed = 0
        for future in as_completed(future_to_file):
            json_file = future_to_file[future]
            completed += 1
            
            try:
                success, message = future.result()
                if success:
                    success_count += 1
                    print(f"[OK] [{completed}/{len(json_files)}] {message}")
                else:
                    print(f"â„¹ï¸  [{completed}/{len(json_files)}] {message}")
            except Exception as e:
                print(f"[NG] [{completed}/{len(json_files)}] Error with {json_file}: {e}")
            
            # é€²è¡ŒçŠ¶æ³è¡¨ç¤º
            if completed % 3 == 0 or completed == len(json_files):
                elapsed = time.time() - start_time
                rate = completed / elapsed if elapsed > 0 else 0
                print(f"[WAIT] Progress: {completed}/{len(json_files)} files | {elapsed:.1f}s | {rate:.2f} files/sec")
    
    # çµæœé›†è¨ˆ
    total_pdfs = len([f for f in os.listdir(pdf_dir) if f.endswith('.pdf')])
    end_time = time.time()
    elapsed = end_time - start_time
    
    print(f"\n[DONE] ResearchGate PDFå–å¾—å®Œäº†!")
    print(f"[CHART] å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’")
    print(f"[DATA] æ–°è¦PDFå–å¾—: {success_count}ä»¶")
    print(f"[DIR] ç·PDFæ•°: {total_pdfs}ä»¶")
    print(f"ğŸ“‚ PDFãƒ•ã‚©ãƒ«ãƒ€: {pdf_dir}")
    print(f"âš¡ å‡¦ç†é€Ÿåº¦: {len(json_files)/elapsed:.2f} files/sec")
    print(f"ğŸ§µ ä¸¦åˆ—åº¦: {max_workers} threads")

if __name__ == "__main__":
    main()