#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
json2tag_ref_scopus_async.py â€• æ”¹è‰¯ç‰ˆ (DOIè§£æ±º500ä»¶ãšã¤åˆ†å‰²å‡¦ç†)
"""

import os, json, re, unicodedata, asyncio
import time, random, hashlib, logging, traceback
from urllib.parse import quote_plus
from typing import Dict, List, Set

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
try:
    import aiohttp, async_timeout
    ASYNC_AVAILABLE = True
    print("âœ… aiohttpåˆ©ç”¨å¯èƒ½ - é«˜é€Ÿä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
except ImportError:
    ASYNC_AVAILABLE = False
    print("âš ï¸  aiohttpæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - æ¨™æº–ç‰ˆã§å®Ÿè¡Œ")
    import urllib.request
    import urllib.error

try:
    import nltk
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
    print("âœ… NLTKåˆ©ç”¨å¯èƒ½ - é«˜åº¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
except ImportError:
    NLTK_AVAILABLE = False
    print("âš ï¸  NLTKæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - åŸºæœ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã®ã¿")

from tqdm import tqdm

# ---------- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ----------
SAFE_ASC = "-_.() " + "".join(chr(c) for c in range(0x30, 0x7B) if chr(c).isalnum())
STOP_POS = {"IN", "CC", "DT", "PRP", "WDT", "WP", "WP$", "VBZ", "VBP", "VBD", "VB", "VBG", "VBN", "RB"}
STOP_TOK = {"am", "is", "are", "was", "were", "be", "being", "been", "not", "to"}
MAX_CONC = 8
DELAY = 0.01
TIMEOUT = 15
MAILTO = "your_email@example.com"
HEAD_X = {"User-Agent": f"mdgen/2.0 (mailto:{MAILTO})"}
CHUNK_SIZE = 500

# ---------- ãƒ­ã‚° ----------
logging.basicConfig(filename="error_log.txt", filemode="a", level=logging.INFO,
                    format="%(asctime)s\tmdgen\t%(levelname)s\t%(message)s")

# ---------- ãƒ˜ãƒ«ãƒ‘é–¢æ•° ----------
def safe_fn(title: str, maxlen: int = 120) -> str:
    norm = unicodedata.normalize("NFKC", title)
    s = "".join(ch for ch in norm if ch in SAFE_ASC)
    s = re.sub(r"\s+", "_", s).strip("_")
    s = re.sub(r"_+", "_", s)[:maxlen]
    return s or hashlib.md5(title.encode()).hexdigest()[:maxlen]

def ensure_nltk():
    """NLTKåˆ©ç”¨å¯èƒ½æ™‚ã®ã¿ãƒªã‚½ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    if not NLTK_AVAILABLE:
        return
        
    for res in [("tokenizers/punkt", "punkt"),
                ("taggers/averaged_perceptron_tagger_eng", "averaged_perceptron_tagger_eng")]:
        try:
            nltk.data.find(res[0])
        except LookupError:
            nltk.download(res[1])

def chunk_list(lst, n):
    """ãƒªã‚¹ãƒˆã‚’ n ä»¶ã”ã¨ã«åˆ†å‰²"""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# ---------- éåŒæœŸ DOI è§£æ±º (aiohttpç‰ˆ) ----------
async def fetch_doi_titles_async(dois: Set[str]) -> Dict[str, str]:
    """éåŒæœŸç‰ˆDOIè§£æ±ºï¼ˆaiohttpä½¿ç”¨ï¼‰"""
    out = {d: "Unknown" for d in dois}
    sem = asyncio.Semaphore(MAX_CONC)

    async with aiohttp.ClientSession() as sess:
        async def fetch_one(doi):
            try:
                async with sem, async_timeout.timeout(TIMEOUT):
                    r = await sess.get(f"https://api.crossref.org/works/{quote_plus(doi)}", headers=HEAD_X)
                    if r.status == 200:
                        js = await r.json()
                        out[doi] = js["message"].get("title", ["Unknown"])[0]
                        return
            except:
                pass
            try:
                async with sem, async_timeout.timeout(TIMEOUT):
                    r = await sess.get(f"https://doi.org/{quote_plus(doi)}",
                                       headers={"Accept": "application/vnd.citationstyles.csl+json", **HEAD_X})
                    if r.status == 200:
                        js = await r.json()
                        out[doi] = js.get("title", "Unknown")
            except:
                pass

        tasks = [fetch_one(d) for d in dois]
        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="DOI è§£æ±º (é«˜é€Ÿç‰ˆ)"):
            await f
            await asyncio.sleep(DELAY)

    return out

# ---------- æ¨™æº–ç‰ˆ DOI è§£æ±º (urllibç‰ˆ) ----------
def fetch_doi_titles_sync(dois: Set[str]) -> Dict[str, str]:
    """æ¨™æº–ç‰ˆDOIè§£æ±ºï¼ˆurllibä½¿ç”¨ï¼‰"""
    out = {d: "Unknown" for d in dois}
    
    for doi in tqdm(dois, desc="DOI è§£æ±º (æ¨™æº–ç‰ˆ)"):
        try:
            # Crossref APIè©¦è¡Œ
            req = urllib.request.Request(
                f"https://api.crossref.org/works/{quote_plus(doi)}", 
                headers=HEAD_X
            )
            with urllib.request.urlopen(req, timeout=TIMEOUT) as response:
                if response.status == 200:
                    js = json.loads(response.read().decode())
                    out[doi] = js["message"].get("title", ["Unknown"])[0]
                    time.sleep(DELAY * 10)  # æ¨™æº–ç‰ˆã¯ã‚ˆã‚Šæ§ãˆã‚ãªé–“éš”
                    continue
        except:
            pass
            
        try:
            # DOI.org APIè©¦è¡Œ
            req = urllib.request.Request(
                f"https://doi.org/{quote_plus(doi)}",
                headers={"Accept": "application/vnd.citationstyles.csl+json", **HEAD_X}
            )
            with urllib.request.urlopen(req, timeout=TIMEOUT) as response:
                if response.status == 200:
                    js = json.loads(response.read().decode())
                    out[doi] = js.get("title", "Unknown")
        except:
            pass
        
        time.sleep(DELAY * 10)  # æ¨™æº–ç‰ˆã¯ã‚ˆã‚Šæ§ãˆã‚ãªé–“éš”
    
    return out

# ---------- DOIè§£æ±ºçµ±åˆé–¢æ•° ----------
def fetch_doi_titles(dois: Set[str]) -> Dict[str, str]:
    """åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å¿œã˜ã¦DOIè§£æ±ºã‚’å®Ÿè¡Œ"""
    if ASYNC_AVAILABLE:
        return asyncio.run(fetch_doi_titles_async(dois))
    else:
        return fetch_doi_titles_sync(dois)

# ---------- ãƒ¡ã‚¤ãƒ³ ----------
def main():
    try:
        print("ğŸš€ json2tag_ref_scopus_async.py ã®å®Ÿè¡Œé–‹å§‹")
        print("=" * 60)
        
        ensure_nltk()
        base = os.path.dirname(os.path.abspath(__file__))
        jdir = os.path.join(base, "JSON_folder")
        mdir = os.path.join(base, "md_folder")
        os.makedirs(mdir, exist_ok=True)

        files = [f for f in os.listdir(jdir) if f.endswith(".json")]
        print(f"ğŸ“ {len(files)} ä»¶ã® JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™")
        
        if not files:
            print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        ref_dois: Set[str] = set()
        for jf in files:
            try:
                data = json.load(open(os.path.join(jdir, jf)))
                for r in data.get("references", []):
                    if isinstance(r, dict) and r.get("DOI"):
                        ref_dois.add(r["DOI"].lower())
            except Exception as e:
                logging.error(f"SCAN_ERR\t{jf}\t{e}")

        doi2title: Dict[str, str] = {}
        if os.path.exists("doi_title_cache.json"):
            doi2title.update(json.load(open("doi_title_cache.json")))

        need = list(ref_dois - doi2title.keys())
        print(f"è§£æ±ºå¿…è¦ DOI æ•°: {len(need)}")

        total_chunks = ((len(need)-1)//CHUNK_SIZE)+1 if need else 0
        print(f"ğŸ“Š DOIè§£æ±ºã‚’{total_chunks}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã¦å‡¦ç†ã—ã¾ã™")
        
        for i, chunk in enumerate(chunk_list(need, CHUNK_SIZE), 1):
            print(f"\nğŸ” === DOI ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} å‡¦ç†ä¸­ ({len(chunk)}ä»¶) ===")
            chunk_start = time.time()
            res = fetch_doi_titles(set(chunk))
            chunk_time = time.time() - chunk_start
            
            # æˆåŠŸ/å¤±æ•—çµ±è¨ˆ
            æˆåŠŸæ•° = sum(1 for v in res.values() if v != "Unknown")
            å¤±æ•—æ•° = len(res) - æˆåŠŸæ•°
            
            doi2title.update(res)
            with open("doi_title_cache.json", "w", encoding="utf-8") as fp:
                json.dump(doi2title, fp, ensure_ascii=False, indent=0)
            
            print(f"âœ… ãƒãƒ£ãƒ³ã‚¯{i}å®Œäº†: æˆåŠŸ{æˆåŠŸæ•°}ä»¶, å¤±æ•—{å¤±æ•—æ•°}ä»¶, æ™‚é–“{chunk_time:.1f}ç§’")
            print(f"ğŸ“ ç´¯è¨ˆè§£æ±ºDOIæ•°: {len([v for v in doi2title.values() if v != 'Unknown'])}")
            
            if i < total_chunks:
                print(f"â³ æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã¾ã§1ç§’å¾…æ©Ÿ...")
                time.sleep(1)

        bar = tqdm(total=len(files), desc="MD ç”Ÿæˆ")
        for jf in files:
            try:
                data = json.load(open(os.path.join(jdir, jf), encoding="utf-8"))
                ttl = data.get("title", "")
                year = data.get("year", "Unknown")
                if not ttl:
                    bar.update(1)
                    continue

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆNLTKåˆ©ç”¨å¯èƒ½æ™‚ã¯é«˜åº¦ãªåˆ†æã€ä¸å¯æ™‚ã¯åŸºæœ¬åˆ†æï¼‰
                if NLTK_AVAILABLE:
                    tags = [t.lower() for t, p in nltk.pos_tag(word_tokenize(ttl))
                            if p not in STOP_POS and t.lower() not in STOP_TOK]
                else:
                    # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå˜èªåˆ†å‰²ã®ã¿ï¼‰
                    words = re.findall(r'\b[a-zA-Z]{3,}\b', ttl.lower())
                    tags = [w for w in words if w not in STOP_TOK]
                tags.append(f"year_{year}")
                md_p = os.path.join(mdir, safe_fn(ttl) + ".md")
                with open(md_p, "w", encoding="utf-8") as fp:
                    fp.write("#" + " #".join(tags))
                    fp.write("\n\n## Abstract\n\n" + data.get("abstract", ""))

                refs = data.get("references", [])
                with open(md_p, "a", encoding="utf-8") as fp:
                    if refs:
                        fp.write("\n\n## å‚è€ƒæ–‡çŒ®\n\n")
                        for r in refs:
                            if isinstance(r, dict):
                                art = r.get("article-title")
                                doi = r.get("DOI", "").lower()
                                if doi:
                                    title = art or doi2title.get(doi, "Unknown")
                                    safe_title = safe_fn(title)
                                    fp.write(f"- DOI: {doi}\n  - [[{safe_title}]]\n")
                                else:
                                    safe_title = safe_fn(art or "Unknown")
                                    fp.write(f"- [[{safe_title}]]\n")
                            else:
                                safe_title = safe_fn(r)
                                fp.write(f"- [[{safe_title}]]\n")
                bar.update(1)
            except Exception as e:
                logging.error(f"MD_ERR\t{jf}\t{e}")
                bar.update(1)

        bar.close()
        
        # æœ€çµ‚çµ±è¨ˆ
        print("\n" + "=" * 60)
        print("ğŸ‰ å‡¦ç†å®Œäº†çµ±è¨ˆ")
        print(f"ğŸ“Š å‡¦ç†ã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")
        print(f"ğŸ“Š è§£æ±ºã—ãŸDOIæ•°: {len([v for v in doi2title.values() if v != 'Unknown'])}")
        print(f"ğŸ“Š ç·DOIæ•°: {len(doi2title)}")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {mdir}")
        
        # ç”Ÿæˆã•ã‚ŒãŸMarkdownãƒ•ã‚¡ã‚¤ãƒ«æ•°ç¢ºèª
        md_count = len([f for f in os.listdir(mdir) if f.endswith('.md')])
        print(f"ğŸ“ ç”ŸæˆMarkdownãƒ•ã‚¡ã‚¤ãƒ«: {md_count}ä»¶")
        print("âœ… Markdownç”Ÿæˆå®Œäº†")
        
    except Exception:
        logging.error(f"FATAL\n{traceback.format_exc()}")
        print(f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚error_log.txt ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()