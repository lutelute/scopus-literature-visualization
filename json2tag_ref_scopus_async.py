#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
json2tag_ref_scopus_async.py â€• æ”¹è‰¯ç‰ˆ (DOIè§£æ±º500ä»¶ãšã¤åˆ†å‰²å‡¦ç†)
"""

import os, json, re, unicodedata, asyncio
import time, random, hashlib, logging, traceback
from urllib.parse import quote_plus
from typing import Dict, List, Set

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

try:
    import aiohttp, async_timeout
    ASYNC_AVAILABLE = True
    print("OK aiohttp available - high-speed parallel mode")
except ImportError:
    ASYNC_AVAILABLE = False
    print("WARN aiohttp not installed - standard mode")
    import urllib.request
    import urllib.error

try:
    import nltk
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
    print("OK NLTK available - advanced keyword analysis")
except ImportError:
    NLTK_AVAILABLE = False
    print("WARN NLTK not installed - basic keyword analysis only")

from tqdm import tqdm

# ---------- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ----------
SAFE_ASC = "-_.() " + "".join(chr(c) for c in range(0x30, 0x7B) if chr(c).isalnum())
STOP_POS = {"IN", "CC", "DT", "PRP", "WDT", "WP", "WP$", "VBZ", "VBP", "VBD", "VB", "VBG", "VBN", "RB"}
STOP_TOK = {"am", "is", "are", "was", "were", "be", "being", "been", "not", "to"}
MAX_CONC = 8
DELAY = 0.01
TIMEOUT = 15
MAILTO = "your_email@example.com"
HEAD_X = {"User-Agent": f"mdgen/2.0 (mailto:{MAILTO})"}
CHUNK_SIZE = 500

# ---------- ãƒ­ã‚° ----------
logging.basicConfig(filename="error_log.txt", filemode="a", level=logging.INFO,
                    format="%(asctime)s\tmdgen\t%(levelname)s\t%(message)s")

# ---------- ãƒ˜ãƒ«ãƒ‘é–¢æ•° ----------
def safe_fn(title: str, maxlen: int = 120) -> str:
    norm = unicodedata.normalize("NFKC", title)
    s = "".join(ch for ch in norm if ch in SAFE_ASC)
    s = re.sub(r"\s+", "_", s).strip("_")
    s = re.sub(r"_+", "_", s)[:maxlen]
    return s or hashlib.md5(title.encode()).hexdigest()[:maxlen]

def extract_title_keywords(title: str) -> List[str]:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰åŒ…æ‹¬çš„ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    if not title:
        return []
    
    keywords = []
    
    # åŸºæœ¬çš„ãªå˜èªæŠ½å‡ºï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
    basic_words = re.findall(r'\b[a-zA-Z]{3,}\b', title.lower())
    filtered_words = [w for w in basic_words if w not in STOP_TOK]
    keywords.extend(filtered_words)
    
    # æŠ€è¡“ç”¨èªãƒ»è¤‡åˆèªã®æŠ½å‡ºï¼ˆãƒã‚¤ãƒ•ãƒ³ã‚„ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã§ç¹‹ãŒã£ãŸèªï¼‰
    compound_words = re.findall(r'\b[a-zA-Z]+[_-][a-zA-Z]+(?:[_-][a-zA-Z]+)*\b', title.lower())
    keywords.extend(compound_words)
    
    # æ•°å€¤ã‚’å«ã‚€å°‚é–€ç”¨èªï¼ˆä¾‹ï¼š5G, CO2, IEEE802ãªã©ï¼‰
    tech_terms = re.findall(r'\b[a-zA-Z]*\d+[a-zA-Z]*\b', title)
    keywords.extend([t.lower() for t in tech_terms if len(t) >= 2])
    
    # å¤§æ–‡å­—ã®ç•¥èªï¼ˆä¾‹ï¼šAI, ML, IoTãªã©ï¼‰
    acronyms = re.findall(r'\b[A-Z]{2,}\b', title)
    keywords.extend([a.lower() for a in acronyms])
    
    return list(set(keywords))  # é‡è¤‡å‰Šé™¤

def create_hashtag_content(tags: List[str]) -> str:
    """ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
    if not tags:
        return ""
    
    # ã‚¿ã‚°ã‚’é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆ#ã‚’ä»˜åŠ ï¼‰
    hashtags = []
    for tag in tags:
        # æ—¢ã«#ã§å§‹ã¾ã£ã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ã€ãã†ã§ãªã‘ã‚Œã°#ã‚’ä»˜åŠ 
        if not tag.startswith('#'):
            hashtags.append(f"#{tag}")
        else:
            hashtags.append(tag)
    
    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’æ”¹è¡Œã§åŒºåˆ‡ã£ã¦è¿”ã™
    return " ".join(hashtags)

def ensure_nltk():
    """NLTKåˆ©ç”¨å¯èƒ½æ™‚ã®ã¿ãƒªã‚½ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    if not NLTK_AVAILABLE:
        return
        
    for res in [("tokenizers/punkt", "punkt"),
                ("tokenizers/punkt_tab", "punkt_tab"),
                ("taggers/averaged_perceptron_tagger_eng", "averaged_perceptron_tagger_eng")]:
        try:
            nltk.data.find(res[0])
        except LookupError:
            nltk.download(res[1])

def chunk_list(lst, n):
    """ãƒªã‚¹ãƒˆã‚’ n ä»¶ã”ã¨ã«åˆ†å‰²"""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# ---------- éåŒæœŸ DOI è§£æ±º (aiohttpç‰ˆ) ----------
async def fetch_doi_titles_async(dois: Set[str]) -> Dict[str, str]:
    """éåŒæœŸç‰ˆDOIè§£æ±ºï¼ˆaiohttpä½¿ç”¨ï¼‰"""
    out = {d: "Unknown" for d in dois}
    sem = asyncio.Semaphore(MAX_CONC)

    async with aiohttp.ClientSession() as sess:
        async def fetch_one(doi):
            try:
                async with sem, async_timeout.timeout(TIMEOUT):
                    r = await sess.get(f"https://api.crossref.org/works/{quote_plus(doi)}", headers=HEAD_X)
                    if r.status == 200:
                        js = await r.json()
                        out[doi] = js["message"].get("title", ["Unknown"])[0]
                        return
            except:
                pass
            try:
                async with sem, async_timeout.timeout(TIMEOUT):
                    r = await sess.get(f"https://doi.org/{quote_plus(doi)}",
                                       headers={"Accept": "application/vnd.citationstyles.csl+json", **HEAD_X})
                    if r.status == 200:
                        js = await r.json()
                        out[doi] = js.get("title", "Unknown")
            except:
                pass

        tasks = [fetch_one(d) for d in dois]
        for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="DOI è§£æ±º (é«˜é€Ÿç‰ˆ)"):
            await f
            await asyncio.sleep(DELAY)

    return out

# ---------- æ¨™æº–ç‰ˆ DOI è§£æ±º (urllibç‰ˆ) ----------
def fetch_doi_titles_sync(dois: Set[str]) -> Dict[str, str]:
    """æ¨™æº–ç‰ˆDOIè§£æ±ºï¼ˆurllibä½¿ç”¨ï¼‰"""
    out = {d: "Unknown" for d in dois}
    
    for doi in tqdm(dois, desc="DOI è§£æ±º (æ¨™æº–ç‰ˆ)"):
        try:
            # Crossref APIè©¦è¡Œ
            req = urllib.request.Request(
                f"https://api.crossref.org/works/{quote_plus(doi)}", 
                headers=HEAD_X
            )
            with urllib.request.urlopen(req, timeout=TIMEOUT) as response:
                if response.status == 200:
                    js = json.loads(response.read().decode())
                    out[doi] = js["message"].get("title", ["Unknown"])[0]
                    time.sleep(DELAY * 10)  # æ¨™æº–ç‰ˆã¯ã‚ˆã‚Šæ§ãˆã‚ãªé–“éš”
                    continue
        except:
            pass
            
        try:
            # DOI.org APIè©¦è¡Œ
            req = urllib.request.Request(
                f"https://doi.org/{quote_plus(doi)}",
                headers={"Accept": "application/vnd.citationstyles.csl+json", **HEAD_X}
            )
            with urllib.request.urlopen(req, timeout=TIMEOUT) as response:
                if response.status == 200:
                    js = json.loads(response.read().decode())
                    out[doi] = js.get("title", "Unknown")
        except:
            pass
        
        time.sleep(DELAY * 10)  # æ¨™æº–ç‰ˆã¯ã‚ˆã‚Šæ§ãˆã‚ãªé–“éš”
    
    return out

# ---------- DOIè§£æ±ºçµ±åˆé–¢æ•° ----------
def fetch_doi_titles(dois: Set[str]) -> Dict[str, str]:
    """åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å¿œã˜ã¦DOIè§£æ±ºã‚’å®Ÿè¡Œ"""
    if ASYNC_AVAILABLE:
        return asyncio.run(fetch_doi_titles_async(dois))
    else:
        return fetch_doi_titles_sync(dois)

# ---------- ãƒ¡ã‚¤ãƒ³ ----------
def main():
    try:
        print("ğŸš€ json2tag_ref_scopus_async.py ã®å®Ÿè¡Œé–‹å§‹")
        print("=" * 60)
        
        ensure_nltk()
        base = os.path.dirname(os.path.abspath(__file__))
        jdir = os.path.join(base, "JSON_folder")
        mdir = os.path.join(base, "md_folder")
        os.makedirs(mdir, exist_ok=True)

        files = [f for f in os.listdir(jdir) if f.endswith(".json")]
        print(f"ğŸ“ {len(files)} ä»¶ã® JSON ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™")
        
        if not files:
            print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        ref_dois: Set[str] = set()
        for jf in files:
            try:
                with open(os.path.join(jdir, jf), encoding="utf-8") as fp:
                    data = json.load(fp)
                for r in data.get("references", []):
                    if isinstance(r, dict) and r.get("DOI"):
                        ref_dois.add(r["DOI"].lower())
            except Exception as e:
                logging.error(f"SCAN_ERR\t{jf}\t{e}")

        doi2title: Dict[str, str] = {}
        if os.path.exists("doi_title_cache.json"):
            with open("doi_title_cache.json", encoding="utf-8") as fp:
                doi2title.update(json.load(fp))

        need = list(ref_dois - doi2title.keys())
        print(f"è§£æ±ºå¿…è¦ DOI æ•°: {len(need)}")

        total_chunks = ((len(need)-1)//CHUNK_SIZE)+1 if need else 0
        print(f"ğŸ“Š DOIè§£æ±ºã‚’{total_chunks}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã¦å‡¦ç†ã—ã¾ã™")
        
        for i, chunk in enumerate(chunk_list(need, CHUNK_SIZE), 1):
            print(f"\nğŸ” === DOI ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} å‡¦ç†ä¸­ ({len(chunk)}ä»¶) ===")
            chunk_start = time.time()
            res = fetch_doi_titles(set(chunk))
            chunk_time = time.time() - chunk_start
            
            # æˆåŠŸ/å¤±æ•—çµ±è¨ˆ
            æˆåŠŸæ•° = sum(1 for v in res.values() if v != "Unknown")
            å¤±æ•—æ•° = len(res) - æˆåŠŸæ•°
            
            doi2title.update(res)
            with open("doi_title_cache.json", "w", encoding="utf-8") as fp:
                json.dump(doi2title, fp, ensure_ascii=False, indent=0)
            
            print(f"âœ… ãƒãƒ£ãƒ³ã‚¯{i}å®Œäº†: æˆåŠŸ{æˆåŠŸæ•°}ä»¶, å¤±æ•—{å¤±æ•—æ•°}ä»¶, æ™‚é–“{chunk_time:.1f}ç§’")
            print(f"ğŸ“ ç´¯è¨ˆè§£æ±ºDOIæ•°: {len([v for v in doi2title.values() if v != 'Unknown'])}")
            
            if i < total_chunks:
                print(f"â³ æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã¾ã§1ç§’å¾…æ©Ÿ...")
                time.sleep(1)

        bar = tqdm(total=len(files), desc="MD ç”Ÿæˆ")
        for jf in files:
            try:
                with open(os.path.join(jdir, jf), encoding="utf-8") as f:
                    data = json.load(f)
                ttl = data.get("title", "")
                year = data.get("year", "Unknown")
                if not ttl:
                    bar.update(1)
                    continue

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆåŒ…æ‹¬çš„ãªåˆ†æï¼‰
                title_keywords = extract_title_keywords(ttl)
                
                # NLTKåˆ©ç”¨å¯èƒ½æ™‚ã¯é«˜åº¦ãªå“è©åˆ†æã‚‚è¿½åŠ 
                nltk_keywords = []
                if NLTK_AVAILABLE:
                    try:
                        nltk_keywords = [t.lower() for t, p in nltk.pos_tag(word_tokenize(ttl))
                                        if p not in STOP_POS and t.lower() not in STOP_TOK]
                    except:
                        pass  # NLTKã‚¨ãƒ©ãƒ¼æ™‚ã¯åŸºæœ¬åˆ†æã®ã¿ä½¿ç”¨
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµ±åˆï¼ˆé‡è¤‡å‰Šé™¤ï¼‰
                all_keywords = list(set(title_keywords + nltk_keywords))
                all_keywords.append(f"year_{year}")
                
                # ã‚¿ã‚°ç”¨ã¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ç”¨ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†ã‘ã‚‹
                tags = all_keywords[:15]  # ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã¯æœ€åˆã®15å€‹ã¾ã§
                hashtag_keywords = all_keywords  # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¯å…¨ã¦ä½¿ç”¨
                md_p = os.path.join(mdir, safe_fn(ttl) + ".md")
                with open(md_p, "w", encoding="utf-8") as fp:
                    # ã‚¿ã‚¤ãƒˆãƒ«è¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã®åŸºæœ¬ã‚¿ã‚°ï¼‰
                    fp.write("#" + " #".join(tags))
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°å½¢å¼ï¼‰
                    hashtag_content = create_hashtag_content(hashtag_keywords)
                    if hashtag_content:
                        fp.write("\n\n## Keywords\n\n" + hashtag_content)
                    
                    # Abstract ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                    fp.write("\n\n## Abstract\n\n" + data.get("abstract", ""))

                refs = data.get("references", [])
                with open(md_p, "a", encoding="utf-8") as fp:
                    if refs:
                        fp.write("\n\n## å‚è€ƒæ–‡çŒ®\n\n")
                        for r in refs:
                            if isinstance(r, dict):
                                art = r.get("article-title")
                                doi = r.get("DOI", "").lower()
                                if doi:
                                    title = art or doi2title.get(doi, "Unknown")
                                    safe_title = safe_fn(title)
                                    fp.write(f"- DOI: {doi}\n  - [[{safe_title}]]\n")
                                else:
                                    safe_title = safe_fn(art or "Unknown")
                                    fp.write(f"- [[{safe_title}]]\n")
                            else:
                                safe_title = safe_fn(r)
                                fp.write(f"- [[{safe_title}]]\n")
                bar.update(1)
            except Exception as e:
                logging.error(f"MD_ERR\t{jf}\t{e}")
                bar.update(1)

        bar.close()
        
        # æœ€çµ‚çµ±è¨ˆ
        print("\n" + "=" * 60)
        print("ğŸ‰ å‡¦ç†å®Œäº†çµ±è¨ˆ")
        print(f"ğŸ“Š å‡¦ç†ã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")
        print(f"ğŸ“Š è§£æ±ºã—ãŸDOIæ•°: {len([v for v in doi2title.values() if v != 'Unknown'])}")
        print(f"ğŸ“Š ç·DOIæ•°: {len(doi2title)}")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {mdir}")
        
        # ç”Ÿæˆã•ã‚ŒãŸMarkdownãƒ•ã‚¡ã‚¤ãƒ«æ•°ç¢ºèª
        md_count = len([f for f in os.listdir(mdir) if f.endswith('.md')])
        print(f"ğŸ“ ç”ŸæˆMarkdownãƒ•ã‚¡ã‚¤ãƒ«: {md_count}ä»¶")
        print("âœ… Markdownç”Ÿæˆå®Œäº†")
        
    except Exception:
        logging.error(f"FATAL\n{traceback.format_exc()}")
        print(f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚error_log.txt ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()